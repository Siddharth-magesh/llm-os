# LLM-OS Default Configuration
# This file contains all default settings for LLM-OS

# =============================================================================
# LLM Provider Settings
# =============================================================================
llm:
  # Default provider to use (ollama, anthropic, openai)
  default_provider: "ollama"

  # Fallback chain when primary provider fails
  fallback_chain:
    - "ollama"
    - "anthropic"
    - "openai"

  # Prefer local models when available
  local_first: true

  # Enable cost optimization (use cheaper models for simple tasks)
  cost_optimization: true

  # Maximum retries before giving up
  max_retries: 3

  # Request timeout in seconds
  timeout: 60

  # Provider-specific settings
  providers:
    ollama:
      enabled: true
      base_url: "http://localhost:11434"
      models:
        fast: "llama3.2:1b"
        default: "llama3.2:3b"
        reasoning: "deepseek-r1:1.5b"
      default_model: "llama3.2:3b"

    anthropic:
      enabled: true
      # API key from environment: ANTHROPIC_API_KEY
      models:
        fast: "claude-3-5-haiku-latest"
        default: "claude-3-5-haiku-latest"
        best: "claude-sonnet-4-20250514"
      default_model: "claude-3-5-haiku-latest"
      max_tokens: 4096

    openai:
      enabled: true
      # API key from environment: OPENAI_API_KEY
      models:
        fast: "gpt-4o-mini"
        default: "gpt-4o-mini"
        best: "gpt-4o"
      default_model: "gpt-4o-mini"
      max_tokens: 4096

# =============================================================================
# Context Management
# =============================================================================
context:
  # Maximum tokens to keep in context
  max_tokens: 8000

  # Maximum messages to keep in history
  max_messages: 100

  # Enable context summarization for long conversations
  enable_summarization: false

  # Save conversation history to disk
  persist_history: true

  # History file location (relative to user config dir)
  history_dir: "history"

# =============================================================================
# MCP (Model Context Protocol) Settings
# =============================================================================
mcp:
  # Directory for system MCP servers
  system_servers_dir: "/usr/lib/llm-os/mcp-servers"

  # Directory for user custom servers
  user_servers_dir: "~/.config/llm-os/mcp-servers"

  # Auto-discover servers in directories
  auto_discover: true

  # Server startup timeout in seconds
  startup_timeout: 10

  # Tool execution timeout in seconds
  tool_timeout: 30

  # Maximum concurrent tool executions
  max_concurrent_tools: 5

  # Servers to start automatically
  auto_start_servers:
    - "filesystem"
    - "applications"
    - "process"
    - "system"

# =============================================================================
# Security Settings
# =============================================================================
security:
  # Require confirmation for destructive operations
  confirm_destructive: true

  # Operations that require confirmation
  dangerous_operations:
    - "delete"
    - "remove"
    - "rm"
    - "format"
    - "shutdown"
    - "reboot"
    - "kill"
    - "uninstall"

  # Sandbox untrusted MCP servers
  sandbox_untrusted: true

  # Enable audit logging
  audit_logging: true

  # Audit log location
  audit_log: "/var/log/llm-os/audit.log"

  # Allowed paths for filesystem operations
  allowed_paths:
    - "~"
    - "/tmp"
    - "/var/tmp"

  # Blocked paths (override allowed)
  blocked_paths:
    - "~/.ssh"
    - "~/.gnupg"
    - "/etc/shadow"
    - "/etc/passwd"

# =============================================================================
# UI Settings
# =============================================================================
ui:
  # Color theme (dark, light, auto)
  theme: "dark"

  # Show status bar
  show_status_bar: true

  # Show token count
  show_token_count: true

  # Enable streaming responses
  streaming: true

  # Typing animation speed (characters per second, 0 = instant)
  typing_speed: 0

  # Show timestamps in conversation
  show_timestamps: false

  # Command history size
  history_size: 1000

  # Enable sound notifications
  sound_notifications: false

# =============================================================================
# Logging Settings
# =============================================================================
logging:
  # Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  level: "INFO"

  # Log to file
  file_logging: true

  # Log file location
  log_file: "/var/log/llm-os/llm-os.log"

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Maximum log file size in MB
  max_size_mb: 100

  # Number of backup files to keep
  backup_count: 5

# =============================================================================
# System Prompt
# =============================================================================
system_prompt: |
  You are the AI core of LLM-OS, a natural language operating system.

  Your role is to translate natural language commands into system operations using the available tools.

  Guidelines:
  1. Be proactive - execute obvious intents without unnecessary questions
  2. Be clear - explain what you're doing before and after
  3. Be safe - confirm before destructive operations
  4. Be helpful - suggest alternatives when something fails
  5. Be context-aware - remember and reference previous commands

  When using tools:
  - Choose the most appropriate tool for the task
  - Handle errors gracefully and suggest alternatives
  - For multi-step tasks, execute steps in sequence
  - Report progress for long-running operations
